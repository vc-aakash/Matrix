{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hugging Face is a great way to get a little bit of a break from the stresses of life. It\\'s a great way to get a little bit of a break from the stresses of life.\\n\\nI\\'m a big fan of the \"I\\'m a big fan of the show\" approach. I\\'ve been a fan of the show for a long time, and I\\'ve always been a fan of the show. I\\'ve been a fan of the show for a long'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"crumb/nano-mistral\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 100}\n",
    ")\n",
    "\n",
    "llm.invoke(\"Hugging Face is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "import huggingface_hub as hf_hub\n",
    "import os\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "hf_hub.login(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an artificial intelligence assistant, answer the question. How does LangChain make LLM application development easier? Easy!\n",
      "\n",
      "LLM is one of the most common applications in a number of technologies. The process of MLing is an efficient way\n"
     ]
    }
   ],
   "source": [
    "template = \"You are an artificial intelligence assistant, answer the question. {question}\"\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template = template\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(    \n",
    "    model_id=\"openai-community/gpt2\",    \n",
    "    task=\"text-generation\"\n",
    ")\n",
    "\n",
    "llm_chain = prompt_template | llm\n",
    "question = \"How does LangChain make LLM application development easier?\"\n",
    "\n",
    "print(llm_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "green, white, orange\n"
     ]
    }
   ],
   "source": [
    "# Turn based\n",
    "\n",
    "from langchain_core.prompts import C\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a geography expert that returns the colors present in a country's flag.\"),\n",
    "        (\"human\", \"France\"),\n",
    "        (\"ai\", \"blue, white, red\"),\n",
    "        (\"human\", \"{country}\")\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key='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')\n",
    "\n",
    "llm_chain = prompt_template | llm\n",
    "\n",
    "country = \"Ireland\"\n",
    "response = llm_chain.invoke({\"country\": country})\n",
    "print(response.content)\n",
    "\n",
    "# Bad results in huggingface openAI gpt2\n",
    "# Huggingface llama not accessible (403)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot Prompting\n",
    "from langchain_core.prompts import FewShotPromptTemplate\n",
    "\n",
    "examples = [\n",
    "  {\n",
    "    \"question\": \"How many DataCamp courses has Jack completed?\",\n",
    "    \"answer\": \"36\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"How much XP does Jack have on DataCamp?\",\n",
    "    \"answer\": \"284,320XP\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What technology does Jack learn about most on DataCamp?\",\n",
    "    \"answer\": \"Python\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack's favorite technology on DataCamp is Python.\n"
     ]
    }
   ],
   "source": [
    "example_prompt = PromptTemplate.from_template(\"Question: {question}\\n{answer}\")\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "llm_chain = prompt_template | llm\n",
    "\n",
    "print(llm_chain.invoke({\"input\": \"What is Jack's favorite technology on DataCamp?\"}).content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
